# info for run
code_root: null
# log to weights and biases
integrations:
  wandb:
    active: False
    project: "sketch_or_face"

## Dataloader info ##
datasets:
  train: data.data_loader.dataset.DatasetTrain
  validation: data.data_loader.dataset.DatasetEval
# info for data loader
data:
  # path to dataset used in the data loader
  #data_root_face: /export/home/rhaecker/documents/sketch2face/data/data_sets/img_align_celeba/
  #data_root_sketch: /export/home/rhaecker/documents/sketch2face/data/data_sets/full_numpy_bitmap_face.npy
  data_root_face: /home/haecker/documents/src/20_ss/deep_vision/sketch2face/data/data_sets/img_align_celeba/
  data_root_sketch: /home/haecker/documents/src/20_ss/deep_vision/sketch2face/data/data_sets/full_numpy_bitmap_face.npy
  # shuffel dataset indices
  shuffle: True
  # 10% of the whole dataset will be the validation dataset
  validation_split: 0.1
  # info for transformation of data
  transform:
    resolution: 64
    mirrow: True
    crop_offset: 10

iterator: iterator.vae_gan.VAE_GAN
model: model.vae_gan.VAE_GAN
# choose model and data type
# either "face" or "sketch"
model_type: "face"
random_seed: 10
# optional Meta Info
explanation: "VAE GAN which reduces the learning rate, predicts sigma, latent dimension 128, trains disciminator only with an accuracy over 70% and no extra convolutions on sketches."
# log level of logger
debug_log_level: False

## Iterator info ##
# specify the batchsize for training
batch_size: 16
# specify the length of training
# in either "num_epochs" or "num_steps" 
num_steps: 5000
#num_epochs: 10

optimization:
  # reduce lr linearly after 70% of training up to zero at the end of training
  reduce_lr: 0.7
  # change the learning rate of the discriminator by this factor in respect roiginal lr of the generator
  D_lr_factor: 1
  # update the discriminator only if the right predictions are below a 70% threshold
  D_accuracy: 0.7

losses:
  # either: "L1", "L2" or "MSE", "BCE", "wasserstein"
  reconstruction_loss: "L2"
  reconstruction_weight: 10
  # loss function applied to discriminator output
  adversarial_loss: "BCE"
  adversarial_weight: 1
  # the weight for the gradient penalty loss
  gp_weight: 0
  # information about the Kullback Leibner Divergence
  kld:
    weight: 1
    # delay the kld loss by 1000 steps
    delay: 1000
    # ramp up the kld loss slowly after the delay within 200 steps
    slope_steps: 200

## Model info ##
learning_rate: 0.0001
# dimension of the latent representation 
latent_dim: 128
# normalize over the batch
batch_norm: True
# how frequent the model is saved
log_freq: 100

variational:
  # should the VAE encoder predict the standard deviation
  sigma: True
  # numb. of linear layers in the bottleneck
  num_latent_layer: 0

# Conv parameters
conv:
  # numb. of channels after first convolution 
  n_channel_start: 32
  # numb. of channels double after every convolution until it reaches "n_channel_max"
  n_channel_max: 128
  # numb. of additional convolutions in every convolutional block which preserve the spatial dimension
  sketch_extra_conv: 0
  face_extra_conv: 0
# bool if the convolutions will use a bias parameters in the encoder and the decoder
bias:
  enc: True
  dec: True
# randomly set a weight to zero with the following probability for the encoder and the decoder
dropout:
  enc_rate: 0
  dec_rate: 0