# info for run
code_root: null
# log to weights and biases
integrations:
  wandb:
    active: False
    project: "sketch2face"

# Dataloader
datasets:
  train: data.data_loader.dataset.DatasetTrain
  validation: data.data_loader.dataset.DatasetEval
# info for data loader
data:
  # path to datasets used in the data loader
  #data_root_face: /export/home/rhaecker/documents/sketch2face/data/data_sets/img_align_celeba/
  #data_root_sketch: /export/home/rhaecker/documents/sketch2face/data/data_sets/full_numpy_bitmap_face.npy
  data_root_face: /home/haecker/documents/src/20_ss/deep_vision/sketch2face/data/data_sets/img_align_celeba/
  data_root_sketch: /home/haecker/documents/src/20_ss/deep_vision/sketch2face/data/data_sets/full_numpy_bitmap_face.npy
  # shuffel dataset indices
  shuffle: True
  # 10% of the whole dataset will be the validation dataset
  validation_split: 0.1
  # info for transformation of data
  transform:
    resolution: 64
    mirrow: True
    crop_offset: 5

# uncomment and specify paths to load pretrained models  
load_models:
  # absolute path to the weights checkpoint for the models, similar like: <path to root>/logs/<run name>/train/checkpoints/model-<last step>.ckpt
  #sketch_path: 
  #face_path: 

iterator: iterator.cycle_wgan.Cycle_WGAN
model: model.cycle_gan.Cycle_WGAN
# the model_type for a cycle gan can only be "sketch2face"
model_type: "sketch2face"
random_seed: 10
# optional Meta Info
explanation: "Cycle wasserstein gan updating discriminators only below 67% accuracy threshold, reducing learning_rate during training and one linear latent layer."
# log level of logger
debug_log_level: False

## Iterator info ##
# specify the batchsize for training
batch_size: 16
# specify the length of training
# in either "num_epochs" or "num_steps" 
num_steps: 5000
#num_epochs: 10

optimization:
  # reduce lr linearly after 70% of training up to zero at the end of training
  reduce_lr: 0.7
  # update the discriminator only if the accuracy of the sketch disciminator and face discriminator is below the first 67% element and second element 67% 
  D_accuracy: [0.67,0.67]
  # change the learning rate of the discriminator by this factor in respect roiginal lr of the generator
  D_lr_factor: 1
  # bool if only the latent layers will be updated during training, if True: config["variational"]["num_latent_layer"] should be bigger than 0
  only_latent_layer: True

losses:
  # either: "L1", "L2" or "MSE", "BCE"
  reconstruction_loss: "L2"
  reconstruction_weight: 10
  # loss function applied to discriminator output
  adversarial_loss: "BCE"
  adversarial_weight: 1
  # the weight for the gradient penalty loss
  gp_weight: 0
  # information about the Kullback Leibner Divergence
  kld:
    weight: 1
    # delay the kld loss by 1000 steps
    delay: 1000
    # ramp up the kld loss slowly after the delay within 200 steps
    slope_steps: 200
  # the generator will only be updated every 5th step. The discriminator will be updated 5 times as much.
  update_disc: 5

## Model info ##
learning_rate: 0.0001
# dimension of the latent representation 
latent_dim: 128
# normalize over the batch
batch_norm: True
# how frequent the model is saved
log_freq: 100

variational:
  # should the VAE encoder predict the standard deviation
  sigma: True
  # numb. of linear layers in the bottleneck
  num_latent_layer: 1

# Conv parameters
conv:
  # numb. of channels after first convolution 
  n_channel_start: 32
  # numb. of channels double after every convolution until it reaches "n_channel_max"
  n_channel_max: 512
  # numb. of additional convolutions in every convolutional block which preserve the spatial dimension
  sketch_extra_conv: 0
  face_extra_conv: 0
# bool if the convolutions will use a bias parameters in the encoder and the decoder
bias:
  enc: True
  dec: True
# randomly set a weight to zero with the following probability for the encoder and the decoder
dropout:
  enc_rate: 0
  dec_rate: 0