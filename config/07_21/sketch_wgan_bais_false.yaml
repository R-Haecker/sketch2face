batch_norm: true
batch_size: 64
bias:
  dec: false
  enc: false
ckpt_freq: null
ckpt_zero: false
code_root: null
conv:
  face_extra_conv: 0
  n_channel_max: 1024
  n_channel_start: 256
  sketch_extra_conv: 0
data:
  data_root_face: /export/home/rhaecker/documents/sketch2face/data/data_sets/img_align_celeba/
  data_root_sketch: /export/home/rhaecker/documents/sketch2face/data/data_sets/full_numpy_bitmap_face.npy
  shuffle: true
  test_split: 0.1
  transform:
    crop_offset: 0
    mirrow: false
    resolution: 32
  validation_split: 0.1
datasets:
  train: data.data_loader.dataset.DatasetTrain
  validation: data.data_loader.dataset.DatasetEval
debug_log_level: False
dropout:
  dec_rate: 0
  disc_rate: 0
  enc_rate: 0
eval_hook:
  eval_callbacks: {}
  eval_op: validation/eval_op
explanation: vae wgan gp L2 with sketch big recon loss.
hook_freq: 1
integrations:
  tensorboard:
    active: false
    handlers:
    - scalars
    - images
    - figures
  wandb:
    active: true
    entity: chacker
    handlers:
    - scalars
    - images
    project: sketch_or_face
iterator: iterator.wgan_vae.Iterator_VAE
latent_dim: 256
learning_rate: 0.0001
log_freq: 1000
log_ops:
- train/log_op
- validation/log_op
losses:
  adversarial_weight: 1.0
  gp_weight: 10
  kld:
    delay: 10000
    slope_steps: 5000
    weight: 5000.0
  reconstruction_loss: L2
  reconstruction_weight: 5000.0
  update_disc: 5
model: model.wgan.CycleWGAN_GP_VAE
model_type: sketch
num_epochs: 18
num_steps: 50000
optimization:
  D_lr_factor: 1
  reduce_lr: 0.2
random_seed: 10
start_log_freq: 1
train_ops:
- train/train_op
variational:
  num_latent_layer: 0
  sigma: true
